"""
Pipeline Chu·∫©n B·ªã D·ªØ Li·ªáu Hu·∫•n Luy·ªán LoRA Phong C√°ch Ghibli
============================================================

Quy tr√¨nh:
1. L·ªçc ·∫£nh c√≥ ng∆∞·ªùi & ƒë·∫∑t t√™n l·∫°i (1.jpg, 2.jpg, ...)
2. Resize v·ªÅ 512x512 pixels
3. Gen caption v·ªõi Gemini API (h·ªó tr·ª£ nhi·ªÅu keys + checkpoint)
4. L∆∞u v√†o data/ghibli/train/
"""

import os
import json
import shutil
from pathlib import Path
from typing import List, Dict, Optional
from PIL import Image
import google.generativeai as genai
from tqdm import tqdm
import time
from dotenv import load_dotenv

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv(os.path.join(os.path.dirname(__file__), ".env"))


# ==================== C·∫§U H√åNH ====================

class Config:
    """C·∫•u h√¨nh pipeline"""
    
    # ƒê∆∞·ªùng d·∫´n
    SOURCE_DIR = r"d:\SE_Data\ghibli_data"
    OUTPUT_DIR = r"d:\SE_Data\data\ghibli\train"
    CHECKPOINT_FILE = r"d:\SE_Data\checkpoint.json"
    
    # K√≠ch th∆∞·ªõc ·∫£nh
    TARGET_SIZE = (512, 512)
    
    # Gemini API
    GEMINI_API_KEYS = [
        os.getenv("GEMINI_API_KEY_1"),
        os.getenv("GEMINI_API_KEY_2"),
        os.getenv("GEMINI_API_KEY_3"),
        os.getenv("GEMINI_API_KEY_4"),
        os.getenv("GEMINI_API_KEY_5"),
    ]
    
    # Model Failover Strategy - Th·ª© t·ª± ∆∞u ti√™n cho m·ªói API key
    MODEL_PRIORITY = [
        "gemini-2.5-flash",           # A.1: Ch·∫•t l∆∞·ª£ng cao, t·ªëc ƒë·ªô t·ªët
        "gemini-2.5-flash-lite",      # A.2: RPD cao h∆°n
        "gemini-2.0-flash",           # A.3: TPM cao h∆°n
        "gemini-2.0-flash-lite",      # A.4: RPM cao nh·∫•t
    ]
    
    # Prompt cho caption
    CAPTION_PROMPT = """
You will receive an image. Describe it in a detailed Ghibli-style caption.
Rules:

Structure: Write the caption as a single descriptive phrase using commas for separation (do not use full stops/periods).
Start with: "Ghibli style". (No colon or commas needed after the starter).
Language Level: Use A2-B1 simple vocabulary and grammar.
Content: Describe age, gender, expression, and clothing. Describe posture or action. Describe the background environment with simple details (light, mood, atmosphere).
Exclusions: Do NOT include any character names, even if recognizable. Never mention Studio Ghibli character names or movie titles.
Length: Make the caption at least 20-30 words (since the structure is limited to one simple sentence).
"""

    # Retry settings for Rate Limit (429)
    MAX_RETRIES_RATE_LIMIT = 5
    INITIAL_BACKOFF = 5  # seconds - Exponential backoff starting point
    MAX_BACKOFF = 64  # seconds - Maximum backoff time


# ==================== B∆Ø·ªöC 1: L·ªåC ·∫¢NH C√ì NG∆Ø·ªúI ====================

class PersonDetector:
    """Ph√°t hi·ªán ·∫£nh c√≥ ng∆∞·ªùi s·ª≠ d·ª•ng MediaPipe ho·∫∑c YOLO"""
    
    def __init__(self):
        self.detector = None
        self._init_detector()
    
    def _init_detector(self):
        """Kh·ªüi t·∫°o detector"""
        try:
            # Th·ª≠ s·ª≠ d·ª•ng MediaPipe tr∆∞·ªõc
            import mediapipe as mp
            self.detector = mp.solutions.pose.Pose(
                static_image_mode=True,
                min_detection_confidence=0.5
            )
            self.detector_type = "mediapipe"
            print("‚úì S·ª≠ d·ª•ng MediaPipe ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi")
        except ImportError:
            try:
                # Fallback sang YOLOv8
                from ultralytics import YOLO
                self.detector = YOLO('yolov8n.pt')
                self.detector_type = "yolo"
                print("‚úì S·ª≠ d·ª•ng YOLOv8 ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi")
            except ImportError:
                print("‚ö† Kh√¥ng t√¨m th·∫•y th∆∞ vi·ªán ph√°t hi·ªán ng∆∞·ªùi. B·ªè qua b∆∞·ªõc l·ªçc.")
                self.detector_type = "none"
    
    def has_person(self, image_path: str) -> bool:
        """Ki·ªÉm tra ·∫£nh c√≥ ng∆∞·ªùi hay kh√¥ng"""
        if self.detector_type == "none":
            return True  # Skip filtering n·∫øu kh√¥ng c√≥ detector
        
        try:
            if self.detector_type == "mediapipe":
                import cv2
                image = cv2.imread(image_path)
                results = self.detector.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
                return results.pose_landmarks is not None
            
            elif self.detector_type == "yolo":
                results = self.detector(image_path, verbose=False)
                # Class 0 l√† 'person' trong COCO dataset
                for r in results:
                    if 0 in r.boxes.cls:
                        return True
                return False
        except Exception as e:
            print(f"‚ö† L·ªói khi ki·ªÉm tra {image_path}: {e}")
            return True  # Gi·ªØ ·∫£nh n·∫øu c√≥ l·ªói


# ==================== B∆Ø·ªöC 2 & 3: X·ª¨ L√ù ·∫¢NH ====================

def resize_image(image_path: str, output_path: str, size: tuple = (512, 512)):
    """
    Resize ·∫£nh v·ªÅ k√≠ch th∆∞·ªõc m·ª•c ti√™u (997x997 -> 512x512)
    
    L∆∞u √Ω: ·∫¢nh g·ªëc ƒë√£ l√† 997√ó997 (square frame t·ª´ auto capture tool),
    n√™n ch·ªâ c·∫ßn resize tr·ª±c ti·∫øp m√† kh√¥ng c·∫ßn center crop.
    
    Args:
        image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh g·ªëc (997√ó997)
        output_path: ƒê∆∞·ªùng d·∫´n l∆∞u ·∫£nh ƒë√£ resize (512√ó512)
        size: K√≠ch th∆∞·ªõc m·ª•c ti√™u (width, height)
    """
    try:
        img = Image.open(image_path)
        
        # Convert sang RGB n·∫øu c·∫ßn
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Resize tr·ª±c ti·∫øp v·ªõi LANCZOS (ch·∫•t l∆∞·ª£ng cao)
        img = img.resize(size, Image.Resampling.LANCZOS)
        
        # L∆∞u
        img.save(output_path, 'JPEG', quality=95)
        return True
        
    except Exception as e:
        print(f"‚ö† L·ªói resize {image_path}: {e}")
        return False


# ==================== B∆Ø·ªöC 4: GEN CAPTION ====================

class GeminiCaptioner:
    """
    T·∫°o caption v·ªõi Gemini API - Model Failover Strategy
    
    Chi·∫øn l∆∞·ª£c:
    1. Rate Limit (429) ‚Üí Exponential Backoff
    2. Quota Exceeded (RPD/TPD) ‚Üí Chuy·ªÉn model ti·∫øp theo
    3. H·∫øt models trong key ‚Üí Chuy·ªÉn API key ti·∫øp theo
    """
    
    def __init__(self, api_keys: List[str], checkpoint_file: str):
        self.api_keys = [key for key in api_keys if key]  # L·ªçc keys r·ªóng
        self.current_key_index = 0
        self.current_model_index = 0
        self.checkpoint_file = checkpoint_file
        self.model = None
        
        if not self.api_keys:
            raise ValueError("Kh√¥ng t√¨m th·∫•y API key n√†o! Vui l√≤ng c·∫•u h√¨nh GEMINI_API_KEY trong .env")
        
        self._init_model()
    
    def _init_model(self):
        """Kh·ªüi t·∫°o model v·ªõi key v√† model hi·ªán t·∫°i"""
        genai.configure(api_key=self.api_keys[self.current_key_index])
        model_name = Config.MODEL_PRIORITY[self.current_model_index]
        self.model = genai.GenerativeModel(model_name)
        
        print(f"‚úì S·ª≠ d·ª•ng API Key #{self.current_key_index + 1}/{len(self.api_keys)} | "
              f"Model: {model_name} ({self.current_model_index + 1}/{len(Config.MODEL_PRIORITY)})")
    
    def _switch_to_next_model(self) -> bool:
        """Chuy·ªÉn sang model ti·∫øp theo trong c√πng API key"""
        self.current_model_index += 1
        
        if self.current_model_index >= len(Config.MODEL_PRIORITY):
            # H·∫øt models, chuy·ªÉn sang key ti·∫øp theo
            return self._switch_to_next_key()
        
        model_name = Config.MODEL_PRIORITY[self.current_model_index]
        print(f"‚ü≥ Chuy·ªÉn sang model: {model_name} (∆∞u ti√™n #{self.current_model_index + 1})")
        self._init_model()
        return True
    
    def _switch_to_next_key(self) -> bool:
        """Chuy·ªÉn sang API key ti·∫øp theo v√† reset model index"""
        self.current_key_index += 1
        
        if self.current_key_index >= len(self.api_keys):
            return False  # H·∫øt t·∫•t c·∫£ keys
        
        # Reset v·ªÅ model ƒë·∫ßu ti√™n khi chuy·ªÉn key
        self.current_model_index = 0
        print(f"\n‚ü≥ Chuy·ªÉn sang API Key #{self.current_key_index + 1}/{len(self.api_keys)}")
        self._init_model()
        return True
    
    def _exponential_backoff(self, attempt: int) -> float:
        """
        T√≠nh th·ªùi gian ch·ªù theo Exponential Backoff
        
        Args:
            attempt: S·ªë l·∫ßn th·ª≠ (0-indexed)
            
        Returns:
            Th·ªùi gian ch·ªù (gi√¢y)
        """
        backoff = min(Config.INITIAL_BACKOFF * (2 ** attempt), Config.MAX_BACKOFF)
        return backoff
    
    def generate_caption(self, image_path: str) -> Optional[str]:
        """
        T·∫°o caption cho ·∫£nh v·ªõi Model Failover Strategy
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh
            
        Returns:
            Caption string ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        for attempt in range(Config.MAX_RETRIES_RATE_LIMIT):
            try:
                # Upload ·∫£nh
                img = Image.open(image_path)
                
                # T·∫°o caption
                response = self.model.generate_content([
                    Config.CAPTION_PROMPT,
                    img
                ])
                
                return response.text.strip()
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # ========== X·ª¨ L√ù BLOCKED PROMPT (Safety Filter) ==========
                if "block_reason" in error_msg or "blocked" in error_msg or "candidates` is empty" in error_msg:
                    print(f"‚ö† ·∫¢nh b·ªã ch·∫∑n b·ªüi safety filter (c√≥ th·ªÉ ch·ª©a n·ªôi dung nh·∫°y c·∫£m): {os.path.basename(image_path)}")
                    return "[BLOCKED_BY_SAFETY_FILTER]"  # ƒê√°nh d·∫•u ƒë·ªÉ b·ªè qua
                
                # ========== X·ª¨ L√ù RATE LIMIT (429) ==========
                elif "429" in error_msg or "rate" in error_msg or "too many requests" in error_msg:
                    if attempt < Config.MAX_RETRIES_RATE_LIMIT - 1:
                        backoff_time = self._exponential_backoff(attempt)
                        print(f"‚è≥ Rate Limit! Ch·ªù {backoff_time:.1f}s tr∆∞·ªõc khi th·ª≠ l·∫°i... "
                              f"(l·∫ßn {attempt + 1}/{Config.MAX_RETRIES_RATE_LIMIT})")
                        time.sleep(backoff_time)
                        continue
                    else:
                        print(f"‚ö† V·∫´n b·ªã Rate Limit sau {Config.MAX_RETRIES_RATE_LIMIT} l·∫ßn th·ª≠")
                        # Chuy·ªÉn model n·∫øu backoff kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c
                        if self._switch_to_next_model():
                            return self.generate_caption(image_path)  # Th·ª≠ l·∫°i v·ªõi model m·ªõi
                        else:
                            return None
                
                # ========== X·ª¨ L√ù H·∫æT QUOTA (RPD/TPD) ==========
                elif "quota" in error_msg or "resource_exhausted" in error_msg or "limit exceeded" in error_msg:
                    print(f"‚ö† Model hi·ªán t·∫°i ƒë√£ h·∫øt quota (RPD/TPD)")
                    
                    if self._switch_to_next_model():
                        # Th·ª≠ l·∫°i ngay v·ªõi model/key m·ªõi
                        return self.generate_caption(image_path)
                    else:
                        print("‚úó ƒê√£ h·∫øt t·∫•t c·∫£ API keys v√† models!")
                        return None
                
                # ========== C√ÅC L·ªñI KH√ÅC (C√ì TH·ªÇ RETRY) ==========
                else:
                    if attempt < Config.MAX_RETRIES_RATE_LIMIT - 1:
                        print(f"‚ö† L·ªói: {e} (th·ª≠ l·∫°i {attempt + 1}/{Config.MAX_RETRIES_RATE_LIMIT})")
                        time.sleep(5)  # Ch·ªù 5 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
                        continue
                    else:
                        print(f"‚úó Kh√¥ng th·ªÉ t·∫°o caption cho {image_path}: {e}")
                        return None
        
        return None
    
    def save_checkpoint(self, data: Dict):
        """L∆∞u checkpoint bao g·ªìm c·∫£ tr·∫°ng th√°i model & key"""
        checkpoint_data = {
            **data,
            'current_key_index': self.current_key_index,
            'current_model_index': self.current_model_index,
            'current_model': Config.MODEL_PRIORITY[self.current_model_index]
        }
        
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def load_checkpoint(self) -> Optional[Dict]:
        """ƒê·ªçc checkpoint v√† kh√¥i ph·ª•c tr·∫°ng th√°i model & key"""
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint = json.load(f)
                
                # Kh√¥i ph·ª•c tr·∫°ng th√°i model & key
                if 'current_key_index' in checkpoint:
                    self.current_key_index = checkpoint['current_key_index']
                if 'current_model_index' in checkpoint:
                    self.current_model_index = checkpoint['current_model_index']
                    self._init_model()  # Kh·ªüi t·∫°o l·∫°i model ƒë√∫ng
                
                return checkpoint
            except Exception as e:
                print(f"‚ö† Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c checkpoint: {e}")
        return None


# ==================== PIPELINE CH√çNH ====================

class GhibliDataPipeline:
    """Pipeline ch√≠nh ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu"""
    
    def __init__(self):
        self.config = Config()
        self.person_detector = PersonDetector()
        self.captioner = None
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
    
    def step1_filter_and_rename(self) -> Dict[str, List[str]]:
        """
        B∆∞·ªõc 1: L·ªçc ·∫£nh c√≥ ng∆∞·ªùi v√† t·ªï ch·ª©c l·∫°i
        
        Returns:
            Dict mapping folder_name -> list of filtered image paths
        """
        print("\n" + "="*60)
        print("B∆Ø·ªöC 1: L·ªåC ·∫¢NH V√Ä ƒê·∫∂T T√äN L·∫†I")
        print("="*60)
        
        filtered_images = {}
        
        # Duy·ªát qua t·ª´ng th∆∞ m·ª•c phim
        movie_folders = [f for f in os.listdir(self.config.SOURCE_DIR) 
                        if os.path.isdir(os.path.join(self.config.SOURCE_DIR, f))]
        
        for folder in sorted(movie_folders):
            folder_path = os.path.join(self.config.SOURCE_DIR, folder)
            print(f"\nüìÅ X·ª≠ l√Ω: {folder}")
            
            # L·∫•y danh s√°ch ·∫£nh
            image_files = [f for f in os.listdir(folder_path) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            
            # L·ªçc ·∫£nh c√≥ ng∆∞·ªùi
            valid_images = []
            for img_file in tqdm(image_files, desc=f"  L·ªçc {folder}"):
                img_path = os.path.join(folder_path, img_file)
                if self.person_detector.has_person(img_path):
                    valid_images.append(img_path)
            
            filtered_images[folder] = valid_images
            print(f"  ‚úì Gi·ªØ l·∫°i {len(valid_images)}/{len(image_files)} ·∫£nh")
        
        return filtered_images
    
    def step2_resize_images(self, filtered_images: Dict[str, List[str]]) -> List[str]:
        """
        B∆∞·ªõc 2: Resize ·∫£nh v√† l∆∞u v·ªõi t√™n m·ªõi
        
        Args:
            filtered_images: Dict t·ª´ b∆∞·ªõc 1
            
        Returns:
            List c√°c ƒë∆∞·ªùng d·∫´n ·∫£nh ƒë√£ resize
        """
        print("\n" + "="*60)
        print("B∆Ø·ªöC 2: RESIZE ·∫¢NH V·ªÄ 512x512")
        print("="*60)
        
        resized_images = []
        counter = 1
        
        for folder, image_paths in filtered_images.items():
            print(f"\nüìÅ Resize: {folder}")
            
            for img_path in tqdm(image_paths, desc=f"  Resize {folder}"):
                output_filename = f"{counter}.jpg"
                output_path = os.path.join(self.config.OUTPUT_DIR, output_filename)
                
                if resize_image(img_path, output_path, self.config.TARGET_SIZE):
                    resized_images.append(output_path)
                    counter += 1
        
        print(f"\n‚úì ƒê√£ resize {len(resized_images)} ·∫£nh")
        return resized_images
    
    def step3_generate_captions(self, image_paths: List[str]):
        """
        B∆∞·ªõc 3: T·∫°o caption cho ·∫£nh
        
        Args:
            image_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n ·∫£nh ƒë√£ resize
        """
        print("\n" + "="*60)
        print("B∆Ø·ªöC 3: T·∫†O CAPTION V·ªöI GEMINI API")
        print("="*60)
        
        # Kh·ªüi t·∫°o captioner
        self.captioner = GeminiCaptioner(
            self.config.GEMINI_API_KEYS,
            self.config.CHECKPOINT_FILE
        )
        
        # ƒê∆∞·ªùng d·∫´n file metadata
        metadata_path = os.path.join(self.config.OUTPUT_DIR, "metadata.jsonl")
        
        # ƒê·ªçc metadata hi·ªán c√≥ ƒë·ªÉ t√¨m ·∫£nh ƒë√£ x·ª≠ l√Ω
        processed_files = set()
        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            entry = json.loads(line)
                            processed_files.add(entry.get('file_name'))
                print(f"‚ü≥ ƒê√£ t√¨m th·∫•y {len(processed_files)} ·∫£nh ƒë√£ c√≥ caption")
            except Exception as e:
                print(f"‚ö† Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c metadata: {e}")
        
        # Ki·ªÉm tra checkpoint
        checkpoint = self.captioner.load_checkpoint()
        start_index = 0
        
        if checkpoint and checkpoint.get('last_processed'):
            start_index = checkpoint['last_processed'] + 1
            print(f"‚ü≥ Checkpoint: ti·∫øp t·ª•c t·ª´ index #{start_index}")
        
        # T·∫°o caption
        with open(metadata_path, 'a', encoding='utf-8') as f:
            for idx in tqdm(range(start_index, len(image_paths)), 
                           desc="  T·∫°o caption", 
                           initial=start_index, 
                           total=len(image_paths)):
                
                img_path = image_paths[idx]
                filename = os.path.basename(img_path)
                
                # B·ªè qua n·∫øu ƒë√£ x·ª≠ l√Ω r·ªìi
                if filename in processed_files:
                    continue
                
                # Gen caption
                caption = self.captioner.generate_caption(img_path)
                
                if caption == "[BLOCKED_BY_SAFETY_FILTER]":
                    # ·∫¢nh b·ªã block - ghi v√†o log v√† b·ªè qua
                    print(f"\n‚äò B·ªè qua {filename} (b·ªã ch·∫∑n b·ªüi safety filter)")
                    # V·∫´n l∆∞u checkpoint ƒë·ªÉ kh√¥ng x·ª≠ l√Ω l·∫°i ·∫£nh n√†y
                    self.captioner.save_checkpoint({
                        'last_processed': idx,
                        'total_images': len(image_paths),
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    })
                    continue
                
                if caption:
                    # L∆∞u v√†o metadata.jsonl
                    metadata_entry = {
                        "file_name": filename,
                        "text": caption
                    }
                    f.write(json.dumps(metadata_entry, ensure_ascii=False) + '\n')
                    f.flush()  # ƒê·∫£m b·∫£o ghi ngay
                    
                    # Th√™m v√†o processed_files
                    processed_files.add(filename)
                    
                    # L∆∞u checkpoint
                    self.captioner.save_checkpoint({
                        'last_processed': idx,
                        'total_images': len(image_paths),
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    })
                else:
                    print(f"\n‚úó Kh√¥ng th·ªÉ t·∫°o caption cho {filename}")
                    # D·ª´ng l·∫°i n·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c caption (l·ªói nghi√™m tr·ªçng)
                    print("‚ö† D·ª´ng pipeline. Vui l√≤ng ki·ªÉm tra API keys v√† ch·∫°y l·∫°i.")
                    return
        
        print(f"\n‚úì Ho√†n th√†nh t·∫°o caption cho {len(image_paths)} ·∫£nh")
        print(f"‚úì Metadata ƒë√£ l∆∞u t·∫°i: {metadata_path}")
        
        # X√≥a checkpoint sau khi ho√†n th√†nh
        if os.path.exists(self.config.CHECKPOINT_FILE):
            os.remove(self.config.CHECKPOINT_FILE)
    
    def run(self, skip_filter: bool = False, skip_resize: bool = False):
        """
        Ch·∫°y to√†n b·ªô pipeline
        
        Args:
            skip_filter: B·ªè qua b∆∞·ªõc l·ªçc ·∫£nh (d√πng khi ƒë√£ l·ªçc r·ªìi)
            skip_resize: B·ªè qua b∆∞·ªõc resize (d√πng khi ƒë√£ resize r·ªìi)
        """
        print("\n" + "="*60)
        print("PIPELINE CHU·∫®N B·ªä D·ªÆ LI·ªÜU GHIBLI LORA")
        print("="*60)
        
        if skip_filter and skip_resize:
            # Ch·ªâ ch·∫°y gen caption
            print("\n‚è≠ B·ªè qua b∆∞·ªõc l·ªçc v√† resize")
            image_paths = sorted([
                os.path.join(self.config.OUTPUT_DIR, f)
                for f in os.listdir(self.config.OUTPUT_DIR)
                if f.lower().endswith(('.jpg', '.jpeg'))
            ], key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
            
            self.step3_generate_captions(image_paths)
        else:
            # Ch·∫°y full pipeline
            if not skip_filter:
                filtered_images = self.step1_filter_and_rename()
            else:
                print("\n‚è≠ B·ªè qua b∆∞·ªõc l·ªçc - Load t·∫•t c·∫£ ·∫£nh t·ª´ SOURCE_DIR")
                # Load t·∫•t c·∫£ ·∫£nh t·ª´ th∆∞ m·ª•c g·ªëc
                filtered_images = {}
                movie_folders = [f for f in os.listdir(self.config.SOURCE_DIR) 
                                if os.path.isdir(os.path.join(self.config.SOURCE_DIR, f))]
                
                for folder in sorted(movie_folders):
                    folder_path = os.path.join(self.config.SOURCE_DIR, folder)
                    image_files = [
                        os.path.join(folder_path, f) 
                        for f in os.listdir(folder_path) 
                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))
                    ]
                    filtered_images[folder] = image_files
                    print(f"  üìÅ {folder}: {len(image_files)} ·∫£nh")
            
            if not skip_resize:
                resized_images = self.step2_resize_images(filtered_images)
            else:
                print("\n‚è≠ B·ªè qua b∆∞·ªõc resize")
                # Load existing images
                resized_images = sorted([
                    os.path.join(self.config.OUTPUT_DIR, f)
                    for f in os.listdir(self.config.OUTPUT_DIR)
                    if f.lower().endswith(('.jpg', '.jpeg'))
                ], key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
            
            self.step3_generate_captions(resized_images)
        
        print("\n" + "="*60)
        print("‚úì HO√ÄN TH√ÄNH PIPELINE!")
        print("="*60)
        print(f"\nD·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {self.config.OUTPUT_DIR}")


# ==================== MAIN ====================

if __name__ == "__main__":
    # T·∫°o pipeline
    pipeline = GhibliDataPipeline()
    
    # Ch·∫°y pipeline
    # T√πy ch·ªânh tham s·ªë n·∫øu c·∫ßn:
    # - skip_filter=True: B·ªè qua b∆∞·ªõc l·ªçc ·∫£nh
    # - skip_resize=True: B·ªè qua b∆∞·ªõc resize (ch·ªâ ch·∫°y gen caption)
    
    # pipeline.run()
    
    # B·ªè qua l·ªçc ·∫£nh, nh∆∞ng v·∫´n resize v√† gen caption:
    # pipeline.run(skip_filter=True, skip_resize=False)
    
    # Ho·∫∑c ch·ªâ ch·∫°y gen caption n·∫øu ƒë√£ c√≥ ·∫£nh:
    pipeline.run(skip_filter=True, skip_resize=True)
